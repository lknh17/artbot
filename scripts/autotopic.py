#!/usr/bin/env python3
"""
è‡ªåŠ¨é€‰é¢˜å¼•æ“ - æ ¸å¿ƒæµç¨‹ï¼š
1. è¯»å–æ¯æ—¥çƒ­ç‚¹æ•°æ®ï¼ˆtrendé¡¹ç›®çš„SQLiteæ•°æ®åº“ï¼‰
2. ç»“åˆæ¯ä¸ªè´¦å·çš„é¢†åŸŸ/äººè®¾ï¼Œç­›é€‰åˆé€‚è¯é¢˜
3. ä¸ºæ¯ä¸ªè´¦å·ç”Ÿæˆæ ‡é¢˜å€™é€‰åˆ—è¡¨
4. æ ¼å¼åŒ–ä¸ºé£ä¹¦ç¡®è®¤æ¶ˆæ¯ æˆ– è‡ªåŠ¨é€‰æ‹©top N
"""
import json
import os
import sqlite3
from datetime import datetime
from pathlib import Path

TREND_DB_DIR = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "trend", "output", "news"
)

PLATFORM_NAMES = {
    "toutiao": "ä»Šæ—¥å¤´æ¡", "baidu": "ç™¾åº¦çƒ­æœ", "weibo": "å¾®åš",
    "zhihu": "çŸ¥ä¹", "bilibili-hot-search": "Bç«™", "douyin": "æŠ–éŸ³",
    "thepaper": "æ¾æ¹ƒæ–°é—»", "wallstreetcn-hot": "åå°”è¡—è§é—»",
    "cls-hot": "è´¢è”ç¤¾", "ifeng": "å‡¤å‡°ç½‘", "tieba": "è´´å§",
}


def _latest_trend_date() -> str:
    """Find the latest available trend db date (YYYY-MM-DD)."""
    try:
        if not os.path.isdir(TREND_DB_DIR):
            return ""
        files = sorted([p.name for p in Path(TREND_DB_DIR).glob("*.db")], reverse=True)
        if not files:
            return ""
        return files[0].replace(".db", "")
    except Exception:
        return ""


def load_today_hot(date: str = None, sources: list = None) -> list:
    """ä» trend æ•°æ®åº“è¯»å–çƒ­ç‚¹åˆ—è¡¨ã€‚

    - é»˜è®¤è¯»å–â€œä»Šå¤©â€çš„åº“ï¼›è‹¥ä»Šå¤©æ²¡æœ‰æ•°æ®ï¼Œåˆ™å›é€€åˆ°æœ€æ–°å¯ç”¨æ—¥æœŸã€‚

    Returns: [{"title": str, "platform": str, "rank": int, "url": str}, ...]
    """
    date = date or datetime.now().strftime("%Y-%m-%d")
    db_path = os.path.join(TREND_DB_DIR, f"{date}.db")
    if not os.path.exists(db_path):
        latest = _latest_trend_date()
        if latest:
            db_path = os.path.join(TREND_DB_DIR, f"{latest}.db")
        if not os.path.exists(db_path):
            return []
    
    db = sqlite3.connect(db_path)
    rows = db.execute(
        "SELECT title, platform_id, rank, url FROM news_items ORDER BY platform_id, rank"
    ).fetchall()
    db.close()
    
    items = []
    for title, pid, rank, url in rows:
        if sources and pid not in sources:
            continue
        items.append({
            "title": title,
            "platform": pid,
            "platform_name": PLATFORM_NAMES.get(pid, pid),
            "rank": rank,
            "url": url or "",
        })
    return items


def filter_hot(items: list, include_kw: list = None, exclude_kw: list = None) -> list:
    """æŒ‰å…³é”®è¯è¿‡æ»¤çƒ­ç‚¹ã€‚"""
    result = items
    if include_kw:
        result = [i for i in result if any(k in i["title"] for k in include_kw)]
    if exclude_kw:
        result = [i for i in result if not any(k in i["title"] for k in exclude_kw)]
    return result


def deduplicate(items: list) -> list:
    """å»é‡ï¼ˆæ ‡é¢˜ç›¸ä¼¼åº¦ > 80% è§†ä¸ºé‡å¤ï¼‰ã€‚ç®€å•å®ç°ï¼šå®Œå…¨ç›¸åŒæ ‡é¢˜å»é‡ã€‚"""
    seen = set()
    result = []
    for item in items:
        t = item["title"].strip()
        if t not in seen:
            seen.add(t)
            result.append(item)
    return result


def match_topics_for_account(hot_items: list, account: dict, count: int = 5) -> list:
    """ä¸ºå•ä¸ªè´¦å·åŒ¹é…åˆé€‚çš„è¯é¢˜ã€‚
    
    æ ¹æ®è´¦å·çš„é¢†åŸŸã€äººè®¾ã€å—ä¼—æ¥æ‰“åˆ†æ’åºã€‚
    Phase 1: ç®€å•å…³é”®è¯åŒ¹é… + rankæƒé‡ã€‚
    åç»­å¯æ¥å…¥ AI è¯­ä¹‰åŒ¹é…ã€‚
    
    Args:
        hot_items: å»é‡åçš„çƒ­ç‚¹åˆ—è¡¨
        account: è´¦å·é…ç½®ï¼ˆå« profile.writing_styleï¼‰
        count: è¿”å›æ•°é‡
    
    Returns: [{"title": str, "platform": str, "score": float, ...}, ...]
    """
    style = (account.get("profile") or {}).get("writing_style") or {}
    domain = style.get("domain", "")
    persona = style.get("persona", "")
    keywords = style.get("keywords") or []
    if isinstance(keywords, str):
        keywords = [k.strip() for k in keywords.split(",") if k.strip()]
    
    # æ‰€æœ‰é¢†åŸŸç›¸å…³å…³é”®è¯
    match_words = keywords + [domain] + [w for w in persona.split() if len(w) >= 2]
    match_words = [w for w in match_words if w]
    
    scored = []
    for item in hot_items:
        title = item["title"]
        score = 0.0
        # Rank bonus: top items get higher base score
        rank = item.get("rank", 50)
        score += max(0, (30 - rank)) * 0.5  # top 1 = 14.5, top 10 = 10
        
        # Keyword match bonus
        for kw in match_words:
            if kw.lower() in title.lower():
                score += 10
        
        # Platform diversity bonus (prefer major platforms)
        major = {"weibo", "baidu", "toutiao", "zhihu", "thepaper"}
        if item["platform"] in major:
            score += 3
        
        scored.append({**item, "score": score})
    
    # Sort by score desc, then rank asc
    scored.sort(key=lambda x: (-x["score"], x["rank"]))
    return scored[:count]


def _load_writer_formulas(writer_key: str = "") -> list:
    """Load title formulas from writers/*.yaml.

    Returns a list of dicts: {type, template, examples?}
    """
    if not writer_key:
        return []
    try:
        import yaml
        writers_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "writers")
        path = os.path.join(writers_dir, f"{writer_key}.yaml")
        if not os.path.exists(path):
            return []
        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        return data.get("title_formulas") or []
    except Exception:
        return []


def _fill_template(tpl: str, values: dict) -> str:
    s = tpl
    for k, v in values.items():
        s = s.replace("{" + k + "}", str(v))
    return s


def _extract_hot_phrase(base: str) -> str:
    """Extract a short phrase from the hot title to anchor the final title."""
    b = (base or "").strip()
    if not b:
        return ""
    # remove common brackets
    for ch in "ã€ã€‘()ï¼ˆï¼‰[]<>ã€Šã€‹â€œâ€\"":
        b = b.replace(ch, "")
    # take before colon-like separators
    for sep in ["ï¼š", ":", "-", "â€”", "|", "ï½œ"]:
        if sep in b:
            b = b.split(sep, 1)[0].strip()
    # clamp: keep shorter to avoid long titles
    return b[:14]


def _trim_title(s: str, max_len: int | None) -> str:
    if not max_len:
        return s
    s = (s or "").strip()
    if len(s) <= max_len:
        return s
    # try to cut at punctuation near max_len
    cut = max_len
    for ch in ["ï¼Œ", "ã€‚", "ï¼š", ":", "ï¼›", ";", "ï¼ˆ", "("]:
        p = s.rfind(ch, 0, max_len)
        if p >= 8:
            cut = p
            break
    return s[:cut].rstrip("ï¼Œã€‚ï¼šï¼›:;ï¼ˆ(").strip()


def _title_hook_score(title: str, domain: str = "", hot_phrase: str = "") -> float:
    t = title or ""
    score = 0.0
    # Hot relevance (must feel related)
    if hot_phrase and hot_phrase in t:
        score += 6

    # Hook signals
    if "ï¼Ÿ" in t or "?" in t:
        score += 2
    if any(x in t for x in ["åˆ«å†", "çœŸç›¸", "åæ‚”", "é¿é›·", "è­¦æƒ•", "ä¸€å®šè¦", "ä½ å¯èƒ½", "å…¶å®", "ä¸ºä»€ä¹ˆ", "åˆ°åº•"]):
        score += 3

    # Numbers perform well on XHS and sometimes MP
    import re
    if re.search(r"\d+", t):
        score += 2

    # Domain relevance
    if domain and domain in t:
        score += 2

    # Penalize overly generic therapy-like titles when no hot anchor
    if not hot_phrase or hot_phrase not in t:
        if any(x in t for x in ["ç„¦è™‘", "å†…è€—", "è‡ªæˆ‘æ€€ç–‘"]):
            score -= 4

    # Keep titles not too long
    if len(t) <= 30:
        score += 1
    if len(t) > 42:
        score -= 2
    return score


def _should_web_search_hot(item: dict, account: dict) -> bool:
    """Heuristic: decide whether a hot topic is worth fetching details.

    We only search a small subset to avoid overfitting toçƒ­ç‚¹ and to control latency.
    """
    try:
        title = (item.get("title") or "").strip()
        url = (item.get("url") or "").strip()
        rank = int(item.get("rank") or 99)
        pid = item.get("platform") or ""
        if not url:
            return False
        # only consider top ranks
        if rank > 5:
            return False
        # keywords that benefit from concrete facts
        kw = [
            "é€šæŠ¥", "å›åº”", "äº‹æ•…", "çˆ†ç‚¸", "èµ·ç«", "è‡´", "æ­»äº¡", "å—ä¼¤",
            "è£å‘˜", "é™è–ª", "åœå·¥", "æ¬ è–ª",
            "æ•™è‚²", "å­¦æ ¡", "è€å¸ˆ", "å­¦ç”Ÿ", "é«˜è€ƒ", "ä¸­è€ƒ",
            "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‰‹æœ¯", "è¯", "ç–«è‹—",
            "æˆ¿è´·", "åˆ©ç‡", "é“¶è¡Œ", "æˆ¿åœ°äº§",
            "æ”¿ç­–", "æ–°è§„", "ç¨", "è¡¥è´´",
        ]
        if any(k in title for k in kw):
            return True
        # major platforms, very top rank: still worth
        if pid in {"weibo", "baidu", "toutiao", "zhihu", "thepaper"} and rank <= 3:
            return True
        return False
    except Exception:
        return False


def generate_title_candidates(account: dict, matched_topics: list) -> list:
    """ä¸ºæ¯ä¸ªåŒ¹é…è¯é¢˜ç”Ÿæˆâ€œæœ€ç»ˆæ–‡ç« æ ‡é¢˜â€å€™é€‰ï¼ˆä¸æ˜¯åŸçƒ­ç‚¹æ ‡é¢˜å¤è¯»ï¼‰ã€‚

    ç›®æ ‡ï¼šç»“åˆè´¦å·äººè®¾/é¢†åŸŸï¼Œç”Ÿæˆæ›´å¸ç›ã€æ›´æƒ³ç‚¹çš„æ ‡é¢˜ã€‚

    Strategy (Phase 1):
    - Use writer.title_formulas if available (writers/*.yaml)
    - Fallback to platform defaults (wechat_mp: dan-koe; xhs: xiaohongshu)
    - Fill templates with lightweight heuristics + account writing_style

    Returns: [{original_title, suggested_title, source, url, score, title_score}, ...]
    """
    profile = account.get("profile") or {}
    ws = profile.get("writing_style") or {}
    domain = ws.get("domain", "")
    persona = ws.get("persona", "")
    platform = account.get("platform", "")

    tc = profile.get("title_config") or {}
    title_len_min = tc.get("len_min")
    title_len_max = tc.get("len_max")
    # Reasonable defaults
    if not title_len_max:
        title_len_max = 28 if platform != "xhs" else 30

    # Determine formula source
    writer_key = profile.get("writer") or ""
    formulas = _load_writer_formulas(writer_key)
    if not formulas:
        # Platform defaults
        formulas = _load_writer_formulas("xiaohongshu" if platform == "xhs" else "dan-koe")

    import random
    results = []

    # Track used patterns to reduce homogeneity inside one run
    used_starts = set()
    used_norms = set()

    for topic in matched_topics:
        base = (topic.get("title") or "").strip()
        hot_phrase = _extract_hot_phrase(base)

        # lightweight values for templates (now includes hot anchoring)
        values = {
            "é¢†åŸŸ": domain or "ç”Ÿæ´»",
            "å†…å®¹": hot_phrase or (base[:10] if base else (domain or "ä¸€ä¸ªçœŸç›¸")),
            "çƒ­ç‚¹": hot_phrase or base[:18],
            "äº‹ä»¶": hot_phrase or base[:18],
            "çŸ­æ—¶é—´": random.choice(["1å¤©", "3å¤©", "7å¤©", "ä¸€ä¸ªå‘¨æœ«", "30åˆ†é’Ÿ"]),
            "å·¨å¤§æˆæœ": random.choice([
                "çœ‹æ¸…å…³é”®å¤„",
                "å°‘èµ°10å¹´å¼¯è·¯",
                "æŠŠå±€é¢æƒ³æ˜ç™½",
                "åˆ«å†è¢«å¸¦èŠ‚å¥",
            ]),
            "å›°å¢ƒ": random.choice(["æƒ…ç»ªèµ·ä¼", "æ€»åœ¨å†…è€—", "è¢«ä¿¡æ¯è£¹æŒŸ", "å¯¹æœªæ¥è¿·èŒ«"]),
            "æ—¶é—´": random.choice(["3ä¸ªæœˆ", "åŠå¹´", "1å¹´"]),
            "è¡ŒåŠ¨": random.choice(["ç›²ç›®ç«™é˜Ÿ", "è·Ÿé£è½¬å‘", "åªçœ‹æƒ…ç»ªä¸çœ‹äº‹å®", "æŠŠé—®é¢˜ç®€å•åŒ–"]),
            "å¸¸è§æ¦‚å¿µ": random.choice(["æƒ…ç»ªç¨³å®š", "åŠªåŠ›", "æˆé•¿", "è‡ªå¾‹"]),
            "å¦å®šè¯": random.choice(["é™·é˜±", "è¯¯åŒº", "å¹»è§‰"]),
            "åå¸¸è¯†è§‚ç‚¹": random.choice(["çœŸæ­£é‡è¦çš„æ˜¯è¯æ®", "å…ˆçœ‹ç»“æ„å†çœ‹æƒ…ç»ª", "åˆ«æ€¥ç€ä¸‹ç»“è®º"]),
            "ç°è±¡": random.choice(["è¶Šåˆ·è¶Šç„¦è™‘", "æ˜æ˜å¾ˆå¿™å´æ²¡æˆæœ", "æ€»è¢«æƒ…ç»ªå¸¦ç€èµ°"]),
            "æ„Ÿå—": random.choice(["æ²‰é»˜", "è­¦é†’", "ä¸é€‚"]),
            "æ•°å­—": random.choice(["3", "5", "7", "10"]),
            "ç‰©å“": random.choice(["æ¸…å•", "æ–¹æ³•", "æ¡†æ¶", "å¿ƒæ³•"]),
            "æ­£é¢è¯„ä»·": random.choice(["æ¸…é†’", "æœ‰ç”¨", "é è°±", "çœå¿ƒ"]),
            "èº«ä»½/ç»å†": random.choice(["èŒåœºäºº", "æ™®é€šäºº", "ç»å†è¿‡ä½è°·çš„äºº", "ä¸­å¹´äºº"]) if persona else random.choice(["æ™®é€šäºº", "èŒåœºäºº"]),
        }

        suggestions = []

        # 1) template-based suggestions (randomize a bit for diversity)
        pool = list(formulas)
        random.shuffle(pool)
        for f in pool[:10]:
            tpl = (f or {}).get("template")
            if not tpl:
                continue
            s = _fill_template(tpl, values)
            s = s.replace("ï¼ï¼", "ï¼").strip()
            # Encourage anchoring to hot phrase
            if hot_phrase and hot_phrase not in s and "{" not in tpl:
                # if template doesn't have placeholders, skip
                pass
            suggestions.append(s)

        # 2) hot-anchored deterministic fallbacks (sample to avoid same pattern every time)
        if hot_phrase:
            fallback_pool = [
                f"{hot_phrase}èƒŒåï¼šæ™®é€šäººæœ€å®¹æ˜“å¿½ç•¥çš„3ä¸ªä¿¡å·",
                f"ä»{hot_phrase}çœ‹ç¤¾ä¼šï¼šåˆ«åªçœ‹çƒ­é—¹ï¼Œè¦çœ‹è¿™ä»¶äº‹çš„ç»“æ„",
                f"{hot_phrase}åˆ·å±ä¹‹åï¼Œæˆ‘æ›´æƒ³æé†’ä½ è¿™1ç‚¹",
                f"{hot_phrase}è¿™ä»¶äº‹ï¼Œæœ€è¯¥é—®çš„å…¶å®æ˜¯ï¼šè°åœ¨æ‰¿æ‹…ä»£ä»·ï¼Ÿ",
                f"å›´è§‚{hot_phrase}æ—¶ï¼Œè¯·å…ˆè®°ä½è¿™æ¡åº•çº¿",
            ]
            suggestions += random.sample(fallback_pool, k=min(2, len(fallback_pool)))
        if base:
            suggestions.append(f"{base}ï¼šåˆ«æ€¥ç€ç«™é˜Ÿï¼Œå…ˆæŠŠå…³é”®ç‚¹çœ‹æ¸…")

        # Rank and pick best 1 per topic (prefer hot-anchored & diverse starts)
        best = None
        best_score = -999
        for s in suggestions:
            # enforce title length preference
            s2 = _trim_title(s, title_len_max)
            ts = _title_hook_score(s2, domain, hot_phrase=hot_phrase)

            # soft preference for min length (avoid too short)
            if title_len_min and len(s2) < int(title_len_min):
                ts -= 2

            start = s2[:10]
            if start in used_starts:
                ts -= 3

            # penalize reused normalized patterns like "{HOT}èƒŒåï¼š..."
            norm = s2
            if hot_phrase and hot_phrase in norm:
                norm = norm.replace(hot_phrase, "{HOT}")
            norm = norm.replace("03ä¸ª", "3ä¸ª")
            if norm in used_norms:
                ts -= 6

            if ts > best_score:
                best_score = ts
                best = s2

        if best:
            used_starts.add(best[:10])
            norm_best = best.replace(hot_phrase, "{HOT}") if hot_phrase else best
            used_norms.add(norm_best)

        results.append({
            "original_title": base,
            "suggested_title": best or base,
            "source": topic.get("platform_name", topic.get("platform", "")),
            "url": topic.get("url", ""),
            "score": topic.get("score", 0),
            "title_score": best_score,
        })

    # Sort by title score first, then topic score
    results.sort(key=lambda x: (-(x.get("title_score") or 0), -(x.get("score") or 0)))
    return results


def run_autotopic(config: dict = None, accounts: list = None) -> dict:
    """æ‰§è¡Œå®Œæ•´çš„è‡ªåŠ¨é€‰é¢˜æµç¨‹ã€‚

    åŠŸèƒ½ï¼š
    - ğŸ”¥ çƒ­ç‚¹ç±»ï¼šåŸºäºè¶‹åŠ¿æ•°æ®æŒ‘é€‰è‹¥å¹²çƒ­ç‚¹ï¼Œå¹¶ç”Ÿæˆâ€œç¬¦åˆè´¦å·äººè®¾â€çš„æ ‡é¢˜å€™é€‰
    - âœ¨ è‡ªä¸»ç±»ï¼šä¸ä¾èµ–çƒ­ç‚¹ï¼ŒåŸºäºè´¦å·å®šä½è‡ªä¸»ç”Ÿæˆæ ‡é¢˜å€™é€‰

    Returns: {
        "accounts": {
            "A": {
                "account_id": str,
                "account_name": str,
                "platform": str,
                "candidates": [...],  # unified 10 titles
            },
        },
        "mode": "manual" | "auto",
        "message": "æ ¼å¼åŒ–çš„é£ä¹¦ç¡®è®¤æ¶ˆæ¯",
    }
    """
    if config is None:
        config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "config", "autotopic.json")
        with open(config_path) as f:
            config = json.load(f)
    
    if accounts is None:
        accounts_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "config", "accounts.json")
        with open(accounts_path) as f:
            accounts = json.load(f).get("accounts", [])
    
    mode = config.get("mode", "manual")

    # New strategy: generate a unified list of candidates (mostly from topic bank,
    # optionally mixed with a few hot-driven titles).
    total_title_count = int(config.get("title_count", 10) or 10)
    hot_mix_count = int(config.get("hot_mix_count", 3) or 3)
    hot_title_count = max(0, min(hot_mix_count, total_title_count))
    bank_title_count = max(0, total_title_count - hot_title_count)

    # In auto mode, how many articles to auto-generate/push per account
    auto_count = int(config.get("auto_count", 3) or 3)
    sources = config.get("hot_sources") or None  # None = all
    include_kw = config.get("filter_keywords") or None
    exclude_kw = config.get("exclude_keywords") or None
    
    # 1. Load hot data (optional)
    hot_items = load_today_hot(sources=sources if sources else None)
    if not hot_items:
        # Allow bank-only mode when trend DB is empty/unavailable.
        hot_items = []

    # 2. Filter (only if we have hot items)
    if hot_items:
        hot_items = filter_hot(hot_items, include_kw, exclude_kw)
        hot_items = deduplicate(hot_items)
        # If filtering wipes out all hot items, still continue with bank-only.
        if not hot_items:
            hot_items = []
    
    # 3. For each enabled account, match topics
    enabled_accounts = [a for a in accounts if a.get("enabled", True)]

    # Recent topic history (avoid repeating same hot titles for an account)
    try:
        project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
        hist_path = os.path.join(project_root, "output", "topic_history.json")
        with open(hist_path, "r", encoding="utf-8") as f:
            topic_history = json.load(f)
    except Exception:
        topic_history = {}
    if not enabled_accounts:
        return {"error": "æ— å¯ç”¨çš„è´¦å·", "accounts": {}, "message": ""}
    
    labels = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    result_accounts = {}
    
    # Helper: try LLM-based title rewrite to better match persona
    def _llm_rewrite_titles(acc: dict, topic_title: str, source_platform: str = "") -> list:
        try:
            from scripts.article_service import build_title_prompt
            from scripts.llm import chat
            prompt = build_title_prompt(acc, topic_title, source_platform=source_platform)
            out = chat(prompt, temperature=0.85, max_tokens=300)
            import re
            lines = []
            for l in out.splitlines():
                l = l.strip()
                if not l:
                    continue
                l = l.strip(" \t-â€¢")
                # remove leading numbering like "1." / "1ã€" / "ï¼ˆ1ï¼‰"
                l = re.sub(r"^\(?\s*\d+\s*[\.ã€)]\s*", "", l).strip()
                l = re.sub(r"^\[\s*\d+\s*\]\s*", "", l).strip()
                if l:
                    lines.append(l)
            return lines[:3]
        except Exception:
            return []

    def _llm_bank_titles(acc: dict, count: int) -> list:
        if count <= 0:
            return []
        try:
            from scripts.topic_banks import load_topic_bank, flatten_atoms
            from scripts.llm import chat
            bank = load_topic_bank(acc.get("id", ""))
            atoms = flatten_atoms(bank)

            ws = (acc.get("profile") or {}).get("writing_style") or {}
            domain = ws.get("domain", "")
            persona = ws.get("persona", "")
            audience = ws.get("audience", "")
            tone = ws.get("tone", "")

            platform = "å…¬ä¼—å·" if acc.get("platform") == "wechat_mp" else "å°çº¢ä¹¦"

            prompt = f"""ä½ æ˜¯ä¸€ä½{platform}å†…å®¹åˆ›ä½œè€…ï¼Œè¯·ä¸ºè´¦å·ç”Ÿæˆ {count} ä¸ªâ€œçˆ†æ¬¾æ½œåŠ›æ ‡é¢˜â€ã€‚

è´¦å·å®šä½ï¼š
- é¢†åŸŸï¼š{domain}
- äººè®¾ï¼š{persona}
- è¯»è€…ï¼š{audience}
- è¯­æ°”ï¼š{tone}

é€‰é¢˜åº“ç´ æï¼ˆå¿…é¡»ä½¿ç”¨å…¶ä¸­çš„å…·ä½“åœºæ™¯/å†²çªæ¥å†™æ ‡é¢˜ï¼Œé¿å…æ³›æ³›ï¼‰ï¼š
- ç—›ç‚¹ï¼š{atoms.get('problems', [])[:24]}
- åœºæ™¯ï¼š{atoms.get('scenes', [])[:24]}
- å†²çªï¼š{atoms.get('conflicts', [])[:24]}
- åŠ¨ä½œï¼š{atoms.get('actions', [])[:24]}

è¦æ±‚ï¼š
1) æ ‡é¢˜æ›´é€‚åˆ 30-40 å²è¯»è€…ï¼ˆå©šå§»/è‚²å„¿/èŒåœº/çˆ¶æ¯/å¥åº·/æˆ¿è´·ç­‰ï¼‰
2) æ¯ä¸ªæ ‡é¢˜å¿…é¡»å¸¦â€œå…·ä½“åœºæ™¯è¯â€æˆ–â€œå†²çªè¯â€ï¼Œç¦æ­¢ç©ºè¯ï¼ˆå¿«èŠ‚å¥æ—¶ä»£/ä¸éš¾å‘ç°/è¶Šæ¥è¶Šâ€¦ï¼‰
3) 10-30 å­—ä¸ºä¸»ï¼Œå°½é‡å£è¯­ã€æœ‰ç«‹åœºï¼ˆä½ ä»¥ä¸º/å…¶å®/åˆ«å†/çœŸæ­£â€¦ï¼‰
4) æ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šã€‚

è¯·è¾“å‡º {count} ä¸ªæ ‡é¢˜ï¼š"""

            out = chat(prompt, temperature=0.9, max_tokens=700)
            import re
            raw = []
            for l in out.splitlines():
                l = l.strip()
                if not l:
                    continue
                l = l.strip(" \t-â€¢")
                l = re.sub(r"^\(?\s*\d+\s*[\.ã€)]\s*", "", l).strip()
                l = re.sub(r"^\[\s*\d+\s*\]\s*", "", l).strip()
                if l:
                    raw.append(l)
            # de-duplicate while keeping order
            seen = set(); uniq = []
            for t in raw:
                if t not in seen:
                    seen.add(t); uniq.append(t)
            return uniq[:count]
        except Exception:
            return []

    for idx, acc in enumerate(enabled_accounts):
        label = labels[idx] if idx < len(labels) else str(idx)

        # ğŸ”¥ Hot candidates (optional)
        hot_candidates = []
        if hot_items and hot_title_count > 0:
            matched = match_topics_for_account(hot_items, acc, count=max(1, hot_title_count) * 3)
            # de-dup with recent topics
            acc_hist = topic_history.get(acc.get("id", ""), {}) if isinstance(topic_history, dict) else {}
            recent_hot = set((acc_hist.get("recent_hot") or [])[:20]) if isinstance(acc_hist, dict) else set()
            filtered = []
            for it in matched:
                if (it.get("title") or "").strip() in recent_hot:
                    continue
                filtered.append(it)
                if len(filtered) >= max(1, hot_title_count):
                    break
            matched = filtered or matched[:max(1, hot_title_count)]

            for t in matched:
                base = (t.get("title") or "").strip()
                source_name = t.get("platform_name", t.get("platform", ""))
                llm_titles = _llm_rewrite_titles(acc, base, source_platform=source_name)
                suggested = llm_titles[0] if llm_titles else None
                if not suggested:
                    # fallback to heuristic generator
                    suggested = (generate_title_candidates(acc, [t]) or [{}])[0].get("suggested_title") or base
                hot_candidates.append({
                    "category": "hot",
                    "original_title": base,
                    "suggested_title": suggested,
                    "source": source_name,
                    "url": t.get("url", ""),
                    "rank": t.get("rank", None),
                    "platform": t.get("platform", ""),
                    "score": t.get("score", 0),
                    "search_suggested": _should_web_search_hot(t, acc),
                })

        # ğŸ“š Bank candidates (topic bank driven)
        bank_candidates = []
        bank_titles = _llm_bank_titles(acc, bank_title_count)
        for bt in bank_titles:
            bank_candidates.append({
                "category": "bank",
                "original_title": "",
                "suggested_title": bt,
                "source": "topic_bank",
                "url": "",
                "rank": None,
                "platform": "",
                "score": 0,
                "search_suggested": False,
            })

        # Unified candidates: mostly bank + a few hot
        candidates = (bank_candidates + hot_candidates)[:total_title_count]

        result_accounts[label] = {
            "account_id": acc.get("id", ""),
            "account_name": acc.get("name", ""),
            "platform": acc.get("platform", ""),
            "candidates": candidates,
        }
    
    # 4. Format message
    if mode == "manual":
        msg = format_manual_message(result_accounts)
    else:
        msg = format_auto_message(result_accounts, auto_count)

    # 5. Persist state for later selection parsing (Feishu replies / web UI)
    # NOTE: cron runner and message delivery rely on this state.
    try:
        project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
        out_dir = os.path.join(project_root, "output")
        os.makedirs(out_dir, exist_ok=True)

        state = {
            "mode": mode,
            "accounts": result_accounts,
            "message": msg,
            "total_hot": len(hot_items),
            "date": datetime.now().strftime("%Y-%m-%d"),
            "updated_at": datetime.now().isoformat(),
        }
        with open(os.path.join(out_dir, "autotopic_state.json"), "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        with open(os.path.join(out_dir, "autotopic_pending_msg.txt"), "w", encoding="utf-8") as f:
            f.write(msg)
    except Exception:
        # best-effort; don't block
        pass

    return {
        "mode": mode,
        "accounts": result_accounts,
        "message": msg,
        "total_hot": len(hot_items),
        "date": datetime.now().strftime("%Y-%m-%d"),
    }


def format_manual_message(accounts: dict) -> str:
    """æ ¼å¼åŒ–äººå·¥ç¡®è®¤æ¶ˆæ¯ã€‚
    
    ç¤ºä¾‹ï¼š
    ğŸ“‹ ä»Šæ—¥é€‰é¢˜å€™é€‰ï¼ˆäººå·¥ç¡®è®¤æ¨¡å¼ï¼‰
    
    A - å…¬ä¼—å·-ç§‘æŠ€å·
    A1. é£Ÿå“ç™¾è´§å…¬å¸ç¢°ç“·å®‡æ ‘è¢«æœ€é«˜æ³•è°´è´£ï¼ˆä»Šæ—¥å¤´æ¡ï¼‰
    A2. Anthropicå‡ºæ‰‹ç¼“è§£AIæ‹…å¿§...ï¼ˆåå°”è¡—è§é—»ï¼‰
    ...
    
    B - å°çº¢ä¹¦-ç”Ÿæ´»å·
    B1. ...
    
    è¯·å›å¤é€‰æ‹©ï¼Œå¦‚ï¼šA1,B3ï¼ˆæ¯ä¸ªè´¦å·é€‰ä¸€ç¯‡ï¼‰
    """
    lines = [f"ğŸ“‹ ä»Šæ—¥é€‰é¢˜å€™é€‰ï¼ˆ{datetime.now().strftime('%mæœˆ%dæ—¥')}ï¼‰\n"]
    
    for label, data in accounts.items():
        acc_name = data["account_name"] or data["account_id"]
        lines.append(f"ã€{label}ã€‘{acc_name}")

        cands = data.get("candidates") or []
        if cands:
            for i, c in enumerate(cands, 1):
                tag = "ğŸ“š" if c.get("category") == "bank" or c.get("source") == "topic_bank" else ("ğŸ”¥" if c.get("category") == "hot" else "")
                source = f"ï¼ˆ{c['source']}ï¼‰" if c.get("source") and c.get("source") not in ("topic_bank", "self") else ""
                ref = f"\n      å‚è€ƒï¼š{c['original_title']}" if c.get("original_title") else ""
                hint = " ğŸ”" if c.get("search_suggested") else ""
                lines.append(f"  {label}{i}. {tag}{c['suggested_title']}{source}{hint}{ref}")
        else:
            lines.append("  ï¼ˆæš‚æ— å€™é€‰ï¼‰")

        lines.append("")
    
    lines.append("ğŸ’¬ è¯·å›å¤é€‰æ‹©ï¼Œå¦‚ï¼šA1,B3ï¼ˆæ¯ä¸ªè´¦å·é€‰ä¸€ç¯‡ï¼Œå³å¼€å§‹ç”Ÿæˆï¼‰")
    return "\n".join(lines)


def format_auto_message(accounts: dict, auto_count: int = 3) -> str:
    """æ ¼å¼åŒ–è‡ªåŠ¨æ¨¡å¼æ¶ˆæ¯ï¼ˆå·²è‡ªåŠ¨é€‰æ‹© top Nï¼Œå¾…ç¡®è®¤æ¨é€ï¼‰ã€‚"""
    lines = [f"ğŸ¤– è‡ªåŠ¨é€‰é¢˜å®Œæˆï¼ˆ{datetime.now().strftime('%mæœˆ%dæ—¥')}ï¼‰\n"]
    lines.append(f"å·²ä¸ºæ¯ä¸ªè´¦å·è‡ªåŠ¨é€‰æ‹© Top {auto_count} è¯é¢˜å¹¶ç”Ÿæˆæ–‡ç« ï¼š\n")
    
    for label, data in accounts.items():
        acc_name = data["account_name"] or data["account_id"]
        lines.append(f"ã€{label}ã€‘{acc_name}")
        for i, c in enumerate(data["candidates"][:auto_count], 1):
            source = f"ï¼ˆ{c['source']}ï¼‰" if c.get("source") else ""
            ref = f"\n      å‚è€ƒï¼š{c['original_title']}" if c.get("original_title") else ""
            lines.append(f"  {label}{i}. {c['suggested_title']}{source}{ref}")
        lines.append("")
    
    lines.append("ğŸ’¬ è¯·å›å¤è¦æ¨é€çš„æ–‡ç« ï¼Œå¦‚ï¼šA1,B2ï¼ˆé€‰å®šçš„å°†æ¨é€åˆ°å¯¹åº”å¹³å°ï¼‰")
    lines.append("å›å¤ã€Œå…¨éƒ¨ã€= å…¨éƒ¨æ¨é€")
    return "\n".join(lines)


def parse_selection(text: str, accounts: dict) -> list:
    """è§£æç”¨æˆ·é€‰æ‹©å›å¤ã€‚
    
    è¾“å…¥å¦‚ "A1,B3" æˆ– "A1 B3" æˆ– "å…¨éƒ¨"
    
    Returns: [{"label": "A", "index": 0, "account_id": str, "title": str}, ...]
    """
    text = text.strip()
    
    if text in ("å…¨éƒ¨", "all", "ALL"):
        result = []
        for label, data in accounts.items():
            for i, c in enumerate(data["candidates"]):
                result.append({
                    "label": label,
                    "index": i,
                    "account_id": data["account_id"],
                    "account_name": data["account_name"],
                    "platform": data["platform"],
                    "title": c["suggested_title"],
                    "url": c.get("url", ""),
                })
        return result
    
    import re
    selections = re.findall(r'([A-Za-z])(\d+)', text)
    result = []
    for label, num in selections:
        label = label.upper()
        idx = int(num) - 1
        if label in accounts and 0 <= idx < len(accounts[label]["candidates"]):
            c = accounts[label]["candidates"][idx]
            result.append({
                "label": label,
                "index": idx,
                "account_id": accounts[label]["account_id"],
                "account_name": accounts[label]["account_name"],
                "platform": accounts[label]["platform"],
                "title": c["suggested_title"],
                "url": c.get("url", ""),
                "original_title": c.get("original_title", ""),
                "source": c.get("source", ""),
                "search_suggested": bool(c.get("search_suggested")),
                "rank": c.get("rank"),
            })
    return result


if __name__ == "__main__":
    # CLI test
    result = run_autotopic()
    if result.get("error"):
        print(f"Error: {result['error']}")
    else:
        print(result["message"])
        print(f"\n--- Total hot items: {result['total_hot']}, Date: {result['date']} ---")
