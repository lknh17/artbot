#!/usr/bin/env python3
"""
è‡ªåŠ¨é€‰é¢˜å¼•æ“ - æ ¸å¿ƒæµç¨‹ï¼š
1. è¯»å–æ¯æ—¥çƒ­ç‚¹æ•°æ®ï¼ˆtrendé¡¹ç›®çš„SQLiteæ•°æ®åº“ï¼‰
2. ç»“åˆæ¯ä¸ªè´¦å·çš„é¢†åŸŸ/äººè®¾ï¼Œç­›é€‰åˆé€‚è¯é¢˜
3. ä¸ºæ¯ä¸ªè´¦å·ç”Ÿæˆæ ‡é¢˜å€™é€‰åˆ—è¡¨
4. æ ¼å¼åŒ–ä¸ºé£ä¹¦ç¡®è®¤æ¶ˆæ¯ æˆ– è‡ªåŠ¨é€‰æ‹©top N
"""
import json
import os
import sqlite3
from datetime import datetime
from pathlib import Path

TREND_DB_DIR = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))),
    "trend", "output", "news"
)

PLATFORM_NAMES = {
    "toutiao": "ä»Šæ—¥å¤´æ¡", "baidu": "ç™¾åº¦çƒ­æœ", "weibo": "å¾®åš",
    "zhihu": "çŸ¥ä¹", "bilibili-hot-search": "Bç«™", "douyin": "æŠ–éŸ³",
    "thepaper": "æ¾æ¹ƒæ–°é—»", "wallstreetcn-hot": "åå°”è¡—è§é—»",
    "cls-hot": "è´¢è”ç¤¾", "ifeng": "å‡¤å‡°ç½‘", "tieba": "è´´å§",
}


def _latest_trend_date() -> str:
    """Find the latest available trend db date (YYYY-MM-DD)."""
    try:
        if not os.path.isdir(TREND_DB_DIR):
            return ""
        files = sorted([p.name for p in Path(TREND_DB_DIR).glob("*.db")], reverse=True)
        if not files:
            return ""
        return files[0].replace(".db", "")
    except Exception:
        return ""


def load_today_hot(date: str = None, sources: list = None) -> list:
    """ä» trend æ•°æ®åº“è¯»å–çƒ­ç‚¹åˆ—è¡¨ã€‚

    - é»˜è®¤è¯»å–â€œä»Šå¤©â€çš„åº“ï¼›è‹¥ä»Šå¤©æ²¡æœ‰æ•°æ®ï¼Œåˆ™å›é€€åˆ°æœ€æ–°å¯ç”¨æ—¥æœŸã€‚

    Returns: [{"title": str, "platform": str, "rank": int, "url": str}, ...]
    """
    date = date or datetime.now().strftime("%Y-%m-%d")
    db_path = os.path.join(TREND_DB_DIR, f"{date}.db")
    if not os.path.exists(db_path):
        latest = _latest_trend_date()
        if latest:
            db_path = os.path.join(TREND_DB_DIR, f"{latest}.db")
        if not os.path.exists(db_path):
            return []

    # normalize sources (aliases)
    src = None
    if sources:
        src = []
        for s in sources:
            s = (s or "").strip()
            if not s:
                continue
            src.append(s)
            if s == "bilibili":
                src.append("bilibili-hot-search")
        # de-dup keep order
        seen = set(); _u=[]
        for s in src:
            if s not in seen:
                seen.add(s); _u.append(s)
        src = _u

    db = sqlite3.connect(db_path)
    rows = db.execute(
        "SELECT title, platform_id, rank, url FROM news_items ORDER BY platform_id, rank"
    ).fetchall()
    db.close()

    items = []
    for title, pid, rank, url in rows:
        if src and pid not in src:
            continue
        items.append({
            "title": title,
            "platform": pid,
            "platform_name": PLATFORM_NAMES.get(pid, pid),
            "rank": rank,
            "url": url or "",
        })

    # If user configured sources but it filtered out everything (misconfig), fallback to all.
    if sources and not items:
        return load_today_hot(date=date, sources=None)

    return items


def filter_hot(items: list, include_kw: list = None, exclude_kw: list = None) -> list:
    """æŒ‰å…³é”®è¯è¿‡æ»¤çƒ­ç‚¹ã€‚"""
    result = items
    if include_kw:
        result = [i for i in result if any(k in i["title"] for k in include_kw)]
    if exclude_kw:
        result = [i for i in result if not any(k in i["title"] for k in exclude_kw)]
    return result


def deduplicate(items: list) -> list:
    """å»é‡ï¼ˆæ ‡é¢˜ç›¸ä¼¼åº¦ > 80% è§†ä¸ºé‡å¤ï¼‰ã€‚ç®€å•å®ç°ï¼šå®Œå…¨ç›¸åŒæ ‡é¢˜å»é‡ã€‚"""
    seen = set()
    result = []
    for item in items:
        t = item["title"].strip()
        if t not in seen:
            seen.add(t)
            result.append(item)
    return result


def match_topics_for_account(hot_items: list, account: dict, count: int = 5) -> list:
    """ä¸ºå•ä¸ªè´¦å·åŒ¹é…åˆé€‚çš„è¯é¢˜ã€‚
    
    æ ¹æ®è´¦å·çš„é¢†åŸŸã€äººè®¾ã€å—ä¼—æ¥æ‰“åˆ†æ’åºã€‚
    Phase 1: ç®€å•å…³é”®è¯åŒ¹é… + rankæƒé‡ã€‚
    åç»­å¯æ¥å…¥ AI è¯­ä¹‰åŒ¹é…ã€‚
    
    Args:
        hot_items: å»é‡åçš„çƒ­ç‚¹åˆ—è¡¨
        account: è´¦å·é…ç½®ï¼ˆå« profile.writing_styleï¼‰
        count: è¿”å›æ•°é‡
    
    Returns: [{"title": str, "platform": str, "score": float, ...}, ...]
    """
    style = (account.get("profile") or {}).get("writing_style") or {}
    domain = style.get("domain", "")
    persona = style.get("persona", "")
    keywords = style.get("keywords") or []
    if isinstance(keywords, str):
        keywords = [k.strip() for k in keywords.split(",") if k.strip()]
    
    # æ‰€æœ‰é¢†åŸŸç›¸å…³å…³é”®è¯
    match_words = keywords + [domain] + [w for w in persona.split() if len(w) >= 2]
    match_words = [w for w in match_words if w]
    
    scored = []
    for item in hot_items:
        title = item["title"]
        score = 0.0
        # Rank bonus: top items get higher base score
        rank = item.get("rank", 50)
        score += max(0, (30 - rank)) * 0.5  # top 1 = 14.5, top 10 = 10
        
        # Keyword match bonus
        for kw in match_words:
            if kw.lower() in title.lower():
                score += 10
        
        # Platform diversity bonus (prefer major platforms)
        major = {"weibo", "baidu", "toutiao", "zhihu", "thepaper"}
        if item["platform"] in major:
            score += 3
        
        scored.append({**item, "score": score})
    
    # Sort by score desc, then rank asc
    scored.sort(key=lambda x: (-x["score"], x["rank"]))
    
    # Platform diversity: limit same platform to max 2 items
    platform_counts = {}
    diverse_result = []
    for item in scored:
        platform = item.get("platform", "unknown")
        if platform_counts.get(platform, 0) >= 2:
            continue
        platform_counts[platform] = platform_counts.get(platform, 0) + 1
        diverse_result.append(item)
        if len(diverse_result) >= count:
            break
    
    # If diversity filter returned too few, fallback to original sorted list
    return diverse_result if len(diverse_result) >= min(count, 3) else scored[:count]


def _load_writer_formulas(writer_key: str = "") -> list:
    """Load title formulas from writers/*.yaml.

    Returns a list of dicts: {type, template, examples?}
    """
    if not writer_key:
        return []
    try:
        import yaml
        writers_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "writers")
        path = os.path.join(writers_dir, f"{writer_key}.yaml")
        if not os.path.exists(path):
            return []
        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        return data.get("title_formulas") or []
    except Exception:
        return []


def _fill_template(tpl: str, values: dict) -> str:
    s = tpl
    for k, v in values.items():
        s = s.replace("{" + k + "}", str(v))
    return s


def _extract_hot_phrase(base: str) -> str:
    """Extract a short phrase from the hot title to anchor the final title."""
    b = (base or "").strip()
    if not b:
        return ""
    # remove common brackets
    for ch in "ã€ã€‘()ï¼ˆï¼‰[]<>ã€Šã€‹â€œâ€\"":
        b = b.replace(ch, "")
    # take before colon-like separators
    for sep in ["ï¼š", ":", "-", "â€”", "|", "ï½œ"]:
        if sep in b:
            b = b.split(sep, 1)[0].strip()
    # clamp: keep shorter to avoid long titles
    return b[:14]


def _trim_title(s: str, max_len: int | None) -> str:
    if not max_len:
        return s
    s = (s or "").strip()
    if len(s) <= max_len:
        return s
    # try to cut at punctuation near max_len
    cut = max_len
    for ch in ["ï¼Œ", "ã€‚", "ï¼š", ":", "ï¼›", ";", "ï¼ˆ", "("]:
        p = s.rfind(ch, 0, max_len)
        if p >= 8:
            cut = p
            break
    return s[:cut].rstrip("ï¼Œã€‚ï¼šï¼›:;ï¼ˆ(").strip()


def _title_hook_score(title: str, domain: str = "", hot_phrase: str = "") -> float:
    t = title or ""
    score = 0.0
    # Hot relevance (must feel related)
    if hot_phrase and hot_phrase in t:
        score += 6

    # Hook signals
    if "ï¼Ÿ" in t or "?" in t:
        score += 2
    if any(x in t for x in ["åˆ«å†", "çœŸç›¸", "åæ‚”", "é¿é›·", "è­¦æƒ•", "ä¸€å®šè¦", "ä½ å¯èƒ½", "å…¶å®", "ä¸ºä»€ä¹ˆ", "åˆ°åº•"]):
        score += 3

    # Numbers perform well on XHS and sometimes MP
    import re
    if re.search(r"\d+", t):
        score += 2

    # Domain relevance
    if domain and domain in t:
        score += 2

    # Penalize overly generic therapy-like titles when no hot anchor
    if not hot_phrase or hot_phrase not in t:
        if any(x in t for x in ["ç„¦è™‘", "å†…è€—", "è‡ªæˆ‘æ€€ç–‘"]):
            score -= 4

    # Keep titles not too long
    if len(t) <= 30:
        score += 1
    if len(t) > 42:
        score -= 2
    return score


def _should_web_search_hot(item: dict, account: dict) -> bool:
    """Heuristic: decide whether a hot topic is worth fetching details.

    We only search a small subset to avoid overfitting toçƒ­ç‚¹ and to control latency.
    """
    try:
        title = (item.get("title") or "").strip()
        url = (item.get("url") or "").strip()
        rank = int(item.get("rank") or 99)
        pid = item.get("platform") or ""
        if not url:
            return False
        # only consider top ranks
        if rank > 5:
            return False
        # keywords that benefit from concrete facts
        kw = [
            "é€šæŠ¥", "å›åº”", "äº‹æ•…", "çˆ†ç‚¸", "èµ·ç«", "è‡´", "æ­»äº¡", "å—ä¼¤",
            "è£å‘˜", "é™è–ª", "åœå·¥", "æ¬ è–ª",
            "æ•™è‚²", "å­¦æ ¡", "è€å¸ˆ", "å­¦ç”Ÿ", "é«˜è€ƒ", "ä¸­è€ƒ",
            "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‰‹æœ¯", "è¯", "ç–«è‹—",
            "æˆ¿è´·", "åˆ©ç‡", "é“¶è¡Œ", "æˆ¿åœ°äº§",
            "æ”¿ç­–", "æ–°è§„", "ç¨", "è¡¥è´´",
        ]
        if any(k in title for k in kw):
            return True
        # major platforms, very top rank: still worth
        if pid in {"weibo", "baidu", "toutiao", "zhihu", "thepaper"} and rank <= 3:
            return True
        return False
    except Exception:
        return False


def generate_title_candidates(account: dict, matched_topics: list) -> list:
    """ä¸ºæ¯ä¸ªåŒ¹é…è¯é¢˜ç”Ÿæˆâ€œæœ€ç»ˆæ–‡ç« æ ‡é¢˜â€å€™é€‰ï¼ˆä¸æ˜¯åŸçƒ­ç‚¹æ ‡é¢˜å¤è¯»ï¼‰ã€‚

    ç›®æ ‡ï¼šç»“åˆè´¦å·äººè®¾/é¢†åŸŸï¼Œç”Ÿæˆæ›´å¸ç›ã€æ›´æƒ³ç‚¹çš„æ ‡é¢˜ã€‚

    Strategy (Phase 1):
    - Use writer.title_formulas if available (writers/*.yaml)
    - Fallback to platform defaults (wechat_mp: dan-koe; xhs: xiaohongshu)
    - Fill templates with lightweight heuristics + account writing_style

    Returns: [{original_title, suggested_title, source, url, score, title_score}, ...]
    """
    profile = account.get("profile") or {}
    ws = profile.get("writing_style") or {}
    domain = ws.get("domain", "")
    persona = ws.get("persona", "")
    platform = account.get("platform", "")

    tc = profile.get("title_config") or {}
    title_len_min = tc.get("len_min")
    title_len_max = tc.get("len_max")
    # Reasonable defaults
    if not title_len_max:
        title_len_max = 28 if platform != "xhs" else 30

    # Determine formula source
    writer_key = profile.get("writer") or ""
    formulas = _load_writer_formulas(writer_key)
    if not formulas:
        # Platform defaults
        formulas = _load_writer_formulas("xiaohongshu" if platform == "xhs" else "dan-koe")

    import random
    results = []

    # Track used patterns to reduce homogeneity inside one run
    used_starts = set()
    used_norms = set()

    for topic in matched_topics:
        base = (topic.get("title") or "").strip()
        hot_phrase = _extract_hot_phrase(base)

        # lightweight values for templates (now includes hot anchoring)
        values = {
            "é¢†åŸŸ": domain or "ç”Ÿæ´»",
            "å†…å®¹": hot_phrase or (base[:10] if base else (domain or "ä¸€ä¸ªçœŸç›¸")),
            "çƒ­ç‚¹": hot_phrase or base[:18],
            "äº‹ä»¶": hot_phrase or base[:18],
            "çŸ­æ—¶é—´": random.choice(["1å¤©", "3å¤©", "7å¤©", "ä¸€ä¸ªå‘¨æœ«", "30åˆ†é’Ÿ"]),
            "å·¨å¤§æˆæœ": random.choice([
                "çœ‹æ¸…å…³é”®å¤„",
                "å°‘èµ°10å¹´å¼¯è·¯",
                "æŠŠå±€é¢æƒ³æ˜ç™½",
                "åˆ«å†è¢«å¸¦èŠ‚å¥",
            ]),
            "å›°å¢ƒ": random.choice(["æƒ…ç»ªèµ·ä¼", "æ€»åœ¨å†…è€—", "è¢«ä¿¡æ¯è£¹æŒŸ", "å¯¹æœªæ¥è¿·èŒ«"]),
            "æ—¶é—´": random.choice(["3ä¸ªæœˆ", "åŠå¹´", "1å¹´"]),
            "è¡ŒåŠ¨": random.choice(["ç›²ç›®ç«™é˜Ÿ", "è·Ÿé£è½¬å‘", "åªçœ‹æƒ…ç»ªä¸çœ‹äº‹å®", "æŠŠé—®é¢˜ç®€å•åŒ–"]),
            "å¸¸è§æ¦‚å¿µ": random.choice(["æƒ…ç»ªç¨³å®š", "åŠªåŠ›", "æˆé•¿", "è‡ªå¾‹"]),
            "å¦å®šè¯": random.choice(["é™·é˜±", "è¯¯åŒº", "å¹»è§‰"]),
            "åå¸¸è¯†è§‚ç‚¹": random.choice(["çœŸæ­£é‡è¦çš„æ˜¯è¯æ®", "å…ˆçœ‹ç»“æ„å†çœ‹æƒ…ç»ª", "åˆ«æ€¥ç€ä¸‹ç»“è®º"]),
            "ç°è±¡": random.choice(["è¶Šåˆ·è¶Šç„¦è™‘", "æ˜æ˜å¾ˆå¿™å´æ²¡æˆæœ", "æ€»è¢«æƒ…ç»ªå¸¦ç€èµ°"]),
            "æ„Ÿå—": random.choice(["æ²‰é»˜", "è­¦é†’", "ä¸é€‚"]),
            "æ•°å­—": random.choice(["3", "5", "7", "10"]),
            "ç‰©å“": random.choice(["æ¸…å•", "æ–¹æ³•", "æ¡†æ¶", "å¿ƒæ³•"]),
            "æ­£é¢è¯„ä»·": random.choice(["æ¸…é†’", "æœ‰ç”¨", "é è°±", "çœå¿ƒ"]),
            "èº«ä»½/ç»å†": random.choice(["èŒåœºäºº", "æ™®é€šäºº", "ç»å†è¿‡ä½è°·çš„äºº", "ä¸­å¹´äºº"]) if persona else random.choice(["æ™®é€šäºº", "èŒåœºäºº"]),
        }

        suggestions = []

        # 1) template-based suggestions (randomize a bit for diversity)
        pool = list(formulas)
        random.shuffle(pool)
        for f in pool[:10]:
            tpl = (f or {}).get("template")
            if not tpl:
                continue
            s = _fill_template(tpl, values)
            s = s.replace("ï¼ï¼", "ï¼").strip()
            # Encourage anchoring to hot phrase
            if hot_phrase and hot_phrase not in s and "{" not in tpl:
                # if template doesn't have placeholders, skip
                pass
            suggestions.append(s)

        # 2) hot-anchored deterministic fallbacks (sample to avoid same pattern every time)
        if hot_phrase:
            fallback_pool = [
                f"{hot_phrase}èƒŒåï¼šæ™®é€šäººæœ€å®¹æ˜“å¿½ç•¥çš„3ä¸ªä¿¡å·",
                f"ä»{hot_phrase}çœ‹ç¤¾ä¼šï¼šåˆ«åªçœ‹çƒ­é—¹ï¼Œè¦çœ‹è¿™ä»¶äº‹çš„ç»“æ„",
                f"{hot_phrase}åˆ·å±ä¹‹åï¼Œæˆ‘æ›´æƒ³æé†’ä½ è¿™1ç‚¹",
                f"{hot_phrase}è¿™ä»¶äº‹ï¼Œæœ€è¯¥é—®çš„å…¶å®æ˜¯ï¼šè°åœ¨æ‰¿æ‹…ä»£ä»·ï¼Ÿ",
                f"å›´è§‚{hot_phrase}æ—¶ï¼Œè¯·å…ˆè®°ä½è¿™æ¡åº•çº¿",
            ]
            suggestions += random.sample(fallback_pool, k=min(2, len(fallback_pool)))
        if base:
            suggestions.append(f"{base}ï¼šåˆ«æ€¥ç€ç«™é˜Ÿï¼Œå…ˆæŠŠå…³é”®ç‚¹çœ‹æ¸…")

        # Rank and pick best 1 per topic (prefer hot-anchored & diverse starts)
        best = None
        best_score = -999
        for s in suggestions:
            # enforce title length preference
            s2 = _trim_title(s, title_len_max)
            ts = _title_hook_score(s2, domain, hot_phrase=hot_phrase)

            # soft preference for min length (avoid too short)
            if title_len_min and len(s2) < int(title_len_min):
                ts -= 2

            start = s2[:10]
            if start in used_starts:
                ts -= 3

            # penalize reused normalized patterns like "{HOT}èƒŒåï¼š..."
            norm = s2
            if hot_phrase and hot_phrase in norm:
                norm = norm.replace(hot_phrase, "{HOT}")
            norm = norm.replace("03ä¸ª", "3ä¸ª")
            if norm in used_norms:
                ts -= 6

            if ts > best_score:
                best_score = ts
                best = s2

        if best:
            used_starts.add(best[:10])
            norm_best = best.replace(hot_phrase, "{HOT}") if hot_phrase else best
            used_norms.add(norm_best)

        results.append({
            "original_title": base,
            "suggested_title": best or base,
            "source": topic.get("platform_name", topic.get("platform", "")),
            "url": topic.get("url", ""),
            "score": topic.get("score", 0),
            "title_score": best_score,
        })

    # Sort by title score first, then topic score
    results.sort(key=lambda x: (-(x.get("title_score") or 0), -(x.get("score") or 0)))
    return results


def run_autotopic(config: dict = None, accounts: list = None) -> dict:
    """æ‰§è¡Œå®Œæ•´çš„è‡ªåŠ¨é€‰é¢˜æµç¨‹ã€‚

    åŠŸèƒ½ï¼š
    - ğŸ”¥ çƒ­ç‚¹ç±»ï¼šåŸºäºè¶‹åŠ¿æ•°æ®æŒ‘é€‰è‹¥å¹²çƒ­ç‚¹ï¼Œå¹¶ç”Ÿæˆâ€œç¬¦åˆè´¦å·äººè®¾â€çš„æ ‡é¢˜å€™é€‰
    - âœ¨ è‡ªä¸»ç±»ï¼šä¸ä¾èµ–çƒ­ç‚¹ï¼ŒåŸºäºè´¦å·å®šä½è‡ªä¸»ç”Ÿæˆæ ‡é¢˜å€™é€‰

    Returns: {
        "accounts": {
            "A": {
                "account_id": str,
                "account_name": str,
                "platform": str,
                "candidates": [...],  # unified 10 titles
            },
        },
        "mode": "manual" | "auto",
        "message": "æ ¼å¼åŒ–çš„é£ä¹¦ç¡®è®¤æ¶ˆæ¯",
    }
    """
    if config is None:
        config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "config", "autotopic.json")
        with open(config_path) as f:
            config = json.load(f)
    
    if accounts is None:
        accounts_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "config", "accounts.json")
        with open(accounts_path) as f:
            accounts = json.load(f).get("accounts", [])
    
    mode = config.get("mode", "manual")

    # New strategy: generate a unified list of candidates (mostly from topic bank,
    # optionally mixed with a few hot-driven titles).
    total_title_count = int((config.get('manual_title_count', 12) if mode == 'manual' else config.get('title_count', 10)) or 10)
    hot_mix_count = int(config.get("hot_mix_count", 5) or 5)
    hot_title_count = max(0, min(hot_mix_count, total_title_count))

    # In auto mode, how many articles to auto-generate/push per account
    auto_count = int(config.get("auto_count", 3) or 3)
    sources = config.get("hot_sources") or None  # None = all
    include_kw = config.get("filter_keywords") or None
    exclude_kw = config.get("exclude_keywords") or None
    
    # 1. Load hot data (optional)
    hot_items = load_today_hot(sources=sources if sources else None)
    if not hot_items:
        # Allow bank-only mode when trend DB is empty/unavailable.
        hot_items = []

    # 2. Filter (only if we have hot items)
    if hot_items:
        hot_items = filter_hot(hot_items, include_kw, exclude_kw)
        hot_items = deduplicate(hot_items)
        # If filtering wipes out all hot items, still continue with bank-only.
        if not hot_items:
            hot_items = []
    
    # 3. For each enabled account, match topics
    enabled_accounts = [a for a in accounts if a.get("enabled", True)]

    # Recent topic history (avoid repeating same hot titles for an account)
    try:
        project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
        hist_path = os.path.join(project_root, "output", "topic_history.json")
        with open(hist_path, "r", encoding="utf-8") as f:
            topic_history = json.load(f)
    except Exception:
        topic_history = {}
    if not enabled_accounts:
        return {"error": "æ— å¯ç”¨çš„è´¦å·", "accounts": {}, "message": ""}
    
    labels = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    result_accounts = {}
    
    # Helper: try LLM-based title rewrite to better match persona
    def _llm_rewrite_titles(acc: dict, topic_title: str, source_platform: str = "") -> list:
        try:
            from scripts.article_service import build_title_prompt
            from scripts.llm import chat
            prompt = build_title_prompt(acc, topic_title, source_platform=source_platform)
            out = chat(prompt, temperature=0.85, max_tokens=300)
            import re
            lines = []
            for l in out.splitlines():
                l = l.strip()
                if not l:
                    continue
                l = l.strip(" \t-â€¢")
                # remove leading numbering like "1." / "1ã€" / "ï¼ˆ1ï¼‰"
                l = re.sub(r"^\(?\s*\d+\s*[\.ã€)]\s*", "", l).strip()
                l = re.sub(r"^\[\s*\d+\s*\]\s*", "", l).strip()
                if l:
                    lines.append(l)
            return lines[:3]
        except Exception:
            return []

    def _llm_bank_titles(acc: dict, count: int) -> list:
        if count <= 0:
            return []
        try:
            from scripts.topic_banks import load_topic_bank, flatten_atoms
            from scripts.llm import chat
            bank = load_topic_bank(acc.get("id", ""))
            atoms = flatten_atoms(bank)

            ws = (acc.get("profile") or {}).get("writing_style") or {}
            domain = ws.get("domain", "")
            persona = ws.get("persona", "")
            audience = ws.get("audience", "")
            tone = ws.get("tone", "")

            platform = "å…¬ä¼—å·" if acc.get("platform") == "wechat_mp" else "å°çº¢ä¹¦"

            prompt = f"""ä½ æ˜¯ä¸€ä½{platform}å†…å®¹åˆ›ä½œè€…ï¼Œè¯·ä¸ºè´¦å·ç”Ÿæˆ {count} ä¸ªâ€œçˆ†æ¬¾æ½œåŠ›æ ‡é¢˜â€ã€‚

è´¦å·å®šä½ï¼š
- é¢†åŸŸï¼š{domain}
- äººè®¾ï¼š{persona}
- è¯»è€…ï¼š{audience}
- è¯­æ°”ï¼š{tone}

é€‰é¢˜åº“ç´ æï¼ˆå‚è€ƒè¿™äº›åœºæ™¯/å†²çªçš„é£æ ¼å’Œè§’åº¦ï¼Œä½†è¯·ç»“åˆå½“å‰ç¤¾ä¼šæƒ…ç»ªè‡ªç”±å‘æ•£ï¼Œä¸è¦å±€é™äºè¿™äº›å…·ä½“ç´ æï¼‰ï¼š
- ç—›ç‚¹ï¼š{atoms.get('problems', [])[:12]}
- åœºæ™¯ï¼š{atoms.get('scenes', [])[:12]}
- å†²çªï¼š{atoms.get('conflicts', [])[:12]}
- åŠ¨ä½œï¼š{atoms.get('actions', [])[:12]}

å‘æ•£è¦æ±‚ï¼š
- ä¸è¦æœºæ¢°ç…§æ¬ä¸Šè¿°ç´ æï¼Œè¦ç»“åˆè´¦å·å®šä½ï¼Œä»"å¥åº·ç„¦è™‘ã€çˆ¶æ¯å…»è€ã€ç¤¾äº¤å…³ç³»ã€è‡ªæˆ‘æˆé•¿"ç­‰æ–°è§’åº¦è‡ªç”±å‘æ•£
- æ¯ä¸ªæ ‡é¢˜å¿…é¡»åŸºäº"å…·ä½“åœºæ™¯"æˆ–"æƒ…ç»ªå†²çª"ï¼Œç¦æ­¢ç©ºæ³›ï¼ˆå¦‚"å¿«èŠ‚å¥æ—¶ä»£""ä¸éš¾å‘ç°""è¶Šæ¥è¶Šâ€¦"ï¼‰
- é¿å…è¿ç»­å¤šå¤©é‡å¤åŒæ ·çš„åœºæ™¯ï¼ˆå¦‚ä¸è¦æ¯å¤©éƒ½æ˜¯"å®¶é•¿ç¾¤""ä¹¦æ¡Œå‰"ï¼‰

æ ¼å¼è¦æ±‚ï¼š
1) æ ‡é¢˜é€‚åˆ 30-45 å²è¯»è€…ï¼ˆå©šå§»/è‚²å„¿/èŒåœº/çˆ¶æ¯/å¥åº·/æˆ¿è´·/ä¸­å¹´è½¬å‹ç­‰ï¼‰
2) 10-22 å­—ä¸ºä¸»ï¼Œå£è¯­åŒ–ã€æœ‰ç«‹åœºï¼ˆä½ ä»¥ä¸º/å…¶å®/åˆ«å†/çœŸæ­£/åˆ°åº•â€¦ï¼‰
3) æ‰€æœ‰æ ‡é¢˜å¿…é¡»æ˜¯ä¸­æ–‡ï¼Œç¦æ­¢è‹±æ–‡æˆ–æ··åˆ
4) æ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šã€‚

è¯·è¾“å‡º {count} ä¸ªæ ‡é¢˜ï¼š"""

            out = chat(prompt, temperature=0.9, max_tokens=700)
            import re
            raw = []
            for l in out.splitlines():
                l = l.strip()
                if not l:
                    continue
                l = l.strip(" \t-â€¢")
                l = re.sub(r"^\(?\s*\d+\s*[\.ã€)]\s*", "", l).strip()
                l = re.sub(r"^\[\s*\d+\s*\]\s*", "", l).strip()
                if l:
                    raw.append(l)
            # de-duplicate while keeping order
            seen = set(); uniq = []
            for t in raw:
                if t not in seen:
                    seen.add(t); uniq.append(t)
            return uniq[:count]
        except Exception:
            return []

    def _bank_titles_no_llm(acc: dict, count: int) -> list:
        """Generate candidate titles from topic bank atoms WITHOUT calling LLM."""
        if count <= 0:
            return []
        try:
            from scripts.topic_banks import load_topic_bank, flatten_atoms
            import random

            bank = load_topic_bank(acc.get('id', ''))
            atoms = flatten_atoms(bank)
            problems = atoms.get('problems') or []
            scenes = atoms.get('scenes') or []
            conflicts = atoms.get('conflicts') or []
            actions = atoms.get('actions') or []

            # Gentle fallbacks if atoms are sparse
            if not scenes:
                scenes = ['å›å®¶çš„è·¯ä¸Š', 'å‘¨æ—¥æ™šä¸Š', 'é¥­æ¡Œä¸Š', 'ç”µæ¢¯é‡Œ', 'å‡Œæ™¨æ‰‹æœºäº®èµ·é‚£ä¸€åˆ»']
            if not problems:
                problems = ['ç„¦è™‘', 'å®³æ€•', 'å†…è€—', 'ä¸å®‰', 'å§”å±ˆ']
            if not conflicts:
                conflicts = ['ä¸æ•¢å¼€å£', 'æ€•è¢«å¦å®š', 'æ€•å…³ç³»æ›´åƒµ', 'ä¸çŸ¥é“æ€ä¹ˆè¯´', 'æ€•è¢«è¯¯è§£']
            if not actions:
                actions = ['å¥½å¥½è¯´', 'æŠŠè¯è¯´æ¸…', 'å…ˆä¼¸æ‰‹', 'å…ˆåœä¸€ä¸‹', 'å…ˆç…§é¡¾è‡ªå·±']

            templates = [
                '{scene}é‚£ä¸€åˆ»ä½ {conflict}ï¼šä½ æ€•çš„ä¸æ˜¯{problem}ï¼Œæ˜¯{action}',
                '{scene}ä¹‹åä½ çªç„¶æ²‰é»˜ï¼šä½ ä»¥ä¸ºåœ¨{action}ï¼Œå…¶å®åœ¨èº²{problem}',
                '{scene}æœ€éš¾çš„ä¸æ˜¯{problem}ï¼Œæ˜¯{conflict}æ€ä¹ˆå¼€å£',
                '{scene}é‡Œé‚£å¥â€œç®—äº†â€ï¼šä½ ä¸æ˜¯è®¤è¾“ï¼Œæ˜¯{conflict}',
                '{scene}ä½ æƒ³{action}ï¼Œå´è¢«{conflict}å¡ä½ï¼šå…ˆåˆ«æ€ªè‡ªå·±',
            ]

            out = []
            for _ in range(count * 4):
                t = random.choice(templates).format(
                    scene=random.choice(scenes),
                    problem=random.choice(problems),
                    conflict=random.choice(conflicts),
                    action=random.choice(actions),
                ).strip()
                if len(t) > 26:
                    t = t[:26]
                if t and t not in out:
                    out.append(t)
                if len(out) >= count:
                    break
            return out[:count]
        except Exception:
            return []


    def _llm_daily_candidates_once(acc: dict, hot_for_prompt: list, hot_count: int, regular_count: int) -> list:
        """Generate today's candidates with ONE LLM call per account.

        Returns a list of candidate dicts with category=hot|bank.
        """
        total = max(0, int(hot_count or 0)) + max(0, int(regular_count or 0))
        if total <= 0:
            return []
        try:
            from scripts.topic_banks import load_topic_bank, flatten_atoms
            from scripts.llm import chat
            import re

            ws = (acc.get('profile') or {}).get('writing_style') or {}
            domain = ws.get('domain', '')
            persona = ws.get('persona', '')
            audience = ws.get('audience', '')
            tone = ws.get('tone', '')
            platform = 'å…¬ä¼—å·' if acc.get('platform') == 'wechat_mp' else 'å°çº¢ä¹¦'

            bank = load_topic_bank(acc.get('id', ''))
            atoms = flatten_atoms(bank)

            hot_lines = []
            for i, it in enumerate((hot_for_prompt or [])[:6], 1):
                title = (it.get('title') or '').strip()
                src = (it.get('platform_name') or it.get('platform') or it.get('source') or '').strip()
                if title:
                    hot_lines.append('{} . {}ï¼ˆ{}ï¼‰'.format(i, title, src))
            hot_part = '\n'.join(hot_lines) if hot_lines else 'ï¼ˆæ— ï¼‰'

            prompt = f"""ä½ æ˜¯ä¸€ä½{platform}å†…å®¹åˆ›ä½œè€…ï¼Œè¯·ä¸ºè´¦å·ç”Ÿæˆä»Šæ—¥å€™é€‰æ ‡é¢˜ï¼šçƒ­ç‚¹ç»“åˆ {hot_count} ä¸ª + å¸¸è§„ {regular_count} ä¸ªã€‚\n\nè´¦å·å®šä½ï¼š\n- é¢†åŸŸï¼š{domain}\n- äººè®¾ï¼š{persona}\n- è¯»è€…ï¼š{audience}\n- è¯­æ°”ï¼š{tone}\n\nä»Šæ—¥çƒ­ç‚¹ï¼ˆä»…ä½œçµæ„Ÿï¼Œä¸å¼ºåˆ¶å†™è¿›æ ‡é¢˜ï¼‰ï¼š\n{hot_part}\n\nè´¦å·é€‰é¢˜ç´ æï¼ˆç”¨äºç”Ÿæˆå…·ä½“ã€ä¸ç©ºæ³›çš„æ ‡é¢˜ï¼‰ï¼š\n- ç—›ç‚¹ï¼š{(atoms.get('problems') or [])[:10]}\n- åœºæ™¯ï¼š{(atoms.get('scenes') or [])[:10]}\n- å†²çªï¼š{(atoms.get('conflicts') or [])[:10]}\n- åŠ¨ä½œï¼š{(atoms.get('actions') or [])[:10]}\n\nè¦æ±‚ï¼š\n1) 10-22å­—ä¸ºä¸»ï¼Œå£è¯­åŒ–ï¼Œæœ‰ç”»é¢/æƒ…ç»ªå†²çª\n2) å…è®¸æé—®/åå·®ï¼šä½ ä»¥ä¸º/å…¶å®/åˆ°åº•/åˆ«å†\n3) ç¦æ­¢ç©ºæ³›å¥ï¼ˆå¿«èŠ‚å¥æ—¶ä»£/ä¸éš¾å‘ç°/è¶Šæ¥è¶Šâ€¦ï¼‰\n4) æ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ä»»ä½•å‰åç¼€ã€‚\n\nå¿…é¡»ä¸¥æ ¼è¾“å‡ºå¯è§£æçš„JSONå¯¹è±¡ï¼ˆä¸è¦Markdown/ä¸è¦è§£é‡Š/ä¸è¦å¤šä½™æ–‡å­—ï¼‰ï¼Œåªè¾“å‡ºJSONæœ¬ä½“ã€‚
JSONå¿…é¡»åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
- hot: æ•°ç»„ï¼Œå…ƒç´ ä¸ºå¯¹è±¡ï¼Œå­—æ®µ original_title ä¸ title
- regular: å­—ç¬¦ä¸²æ•°ç»„
æ•°é‡è¦æ±‚ï¼šhot=hot_countï¼Œregular=regular_countã€‚
"""

            out = chat(prompt, temperature=0.75, max_tokens=900)
            import json as _json
            try:
                data = _json.loads(out)
            except Exception:
                # fallback: try extract json object
                m2 = re.search(r"\{[\s\S]*\}", out)
                data = _json.loads(m2.group(0)) if m2 else {"hot": [], "regular": []}

            hot = data.get('hot') or []
            regular = data.get('regular') or []

            candidates = []
            # hot candidates
            for it in hot[:max(0, int(hot_count or 0))]:
                ot = (it.get('original_title') or '').strip() if isinstance(it, dict) else ''
                tt = (it.get('title') or '').strip() if isinstance(it, dict) else ''
                if tt:
                    candidates.append({
                        'category': 'hot',
                        'original_title': ot,
                        'suggested_title': tt,
                        'source': (hot_for_prompt[0].get('platform_name') or hot_for_prompt[0].get('platform') or hot_for_prompt[0].get('source') or '') if hot_for_prompt else '',
                        'url': '',
                        'rank': None,
                        'platform': '',
                        'score': 0,
                        'search_suggested': True,
                    })

            # regular candidates
            # normalize regular as strings
            reg_lines = []
            if isinstance(regular, list):
                for l in regular:
                    if isinstance(l, str) and l.strip():
                        reg_lines.append(l.strip())
            # de-dup keep order
            seen=set()
            reg_uniq=[]
            for t in reg_lines:
                if t not in seen:
                    seen.add(t)
                    reg_uniq.append(t)

            for t in reg_uniq[:max(0, int(regular_count or 0))]:
                candidates.append({
                    'category': 'bank',
                    'original_title': '',
                    'suggested_title': t,
                    'source': 'topic_bank',
                    'url': '',
                    'rank': None,
                    'platform': '',
                    'score': 0,
                    'search_suggested': False,
                })

            return candidates

        except Exception:
            return []


    for idx, acc in enumerate(enabled_accounts):
        label = labels[idx] if idx < len(labels) else str(idx)

        # One-call strategy: generate today's titles with a single LLM call per account.
        # This replaces per-hot rewrite (N calls) + bank brainstorming (1 call).
        hot_for_prompt = hot_items[:max(0, hot_title_count)] if hot_items else []
        regular_count = max(0, total_title_count - max(0, hot_title_count))

        candidates = _llm_daily_candidates_once(acc, hot_for_prompt, hot_title_count, regular_count)
        if not candidates:
            # fallback: generate regular titles without LLM
            titles = _bank_titles_no_llm(acc, total_title_count)
            candidates = [{
                'category': 'bank',
                'original_title': '',
                'suggested_title': t,
                'source': 'topic_bank',
                'url': '',
                'rank': None,
                'platform': '',
                'score': 0,
                'search_suggested': False,
            } for t in titles]

        result_accounts[label] = {
            "account_id": acc.get("id", ""),
            "account_name": acc.get("name", ""),
            "platform": acc.get("platform", ""),
            "candidates": candidates,
        }
    
    # 4. Format message
    if mode == "manual":
        msg = format_manual_message(result_accounts)
    else:
        msg = format_auto_message(result_accounts, auto_count)

    # 5. Persist state for later selection parsing (Feishu replies / web UI)
    # NOTE: cron runner and message delivery rely on this state.
    try:
        project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
        out_dir = os.path.join(project_root, "output")
        os.makedirs(out_dir, exist_ok=True)

        state = {
            "mode": mode,
            "accounts": result_accounts,
            "message": msg,
            "total_hot": len(hot_items),
            "date": datetime.now().strftime("%Y-%m-%d"),
            "updated_at": datetime.now().isoformat(),
        }
        with open(os.path.join(out_dir, "autotopic_state.json"), "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        with open(os.path.join(out_dir, "autotopic_pending_msg.txt"), "w", encoding="utf-8") as f:
            f.write(msg)
    except Exception:
        # best-effort; don't block
        pass

    return {
        "mode": mode,
        "accounts": result_accounts,
        "message": msg,
        "total_hot": len(hot_items),
        "date": datetime.now().strftime("%Y-%m-%d"),
    }


def format_manual_message(accounts: dict) -> str:
    """æ ¼å¼åŒ–äººå·¥ç¡®è®¤æ¶ˆæ¯ã€‚
    
    ç¤ºä¾‹ï¼š
    ğŸ“‹ ä»Šæ—¥é€‰é¢˜å€™é€‰ï¼ˆäººå·¥ç¡®è®¤æ¨¡å¼ï¼‰
    
    A - å…¬ä¼—å·-ç§‘æŠ€å·
    A1. é£Ÿå“ç™¾è´§å…¬å¸ç¢°ç“·å®‡æ ‘è¢«æœ€é«˜æ³•è°´è´£ï¼ˆä»Šæ—¥å¤´æ¡ï¼‰
    A2. Anthropicå‡ºæ‰‹ç¼“è§£AIæ‹…å¿§...ï¼ˆåå°”è¡—è§é—»ï¼‰
    ...
    
    B - å°çº¢ä¹¦-ç”Ÿæ´»å·
    B1. ...
    
    è¯·å›å¤é€‰æ‹©ï¼Œå¦‚ï¼šA1,B3ï¼ˆæ¯ä¸ªè´¦å·é€‰ä¸€ç¯‡ï¼‰
    """
    lines = [f"ğŸ“‹ ä»Šæ—¥é€‰é¢˜å€™é€‰ï¼ˆ{datetime.now().strftime('%mæœˆ%dæ—¥')}ï¼‰\n"]
    
    for label, data in accounts.items():
        acc_name = data["account_name"] or data["account_id"]
        lines.append(f"ã€{label}ã€‘{acc_name}")

        cands = data.get("candidates") or []
        if cands:
            for i, c in enumerate(cands, 1):
                tag = "ğŸ“š" if c.get("category") == "bank" or c.get("source") == "topic_bank" else ("ğŸ”¥" if c.get("category") == "hot" else "")
                source = f"ï¼ˆ{c['source']}ï¼‰" if c.get("source") and c.get("source") not in ("topic_bank", "self") else ""
                ref = f"\n      å‚è€ƒï¼š{c['original_title']}" if c.get("original_title") else ""
                hint = " ğŸ”" if c.get("search_suggested") else ""
                hot_mark = "ã€çƒ­ç‚¹ç»“åˆï¼šæ— ã€‘"
                if c.get("category") == "hot" and c.get("original_title"):
                    hot_mark = f"ã€çƒ­ç‚¹ç»“åˆï¼š{c.get('source','')}ï½œ{c.get('original_title','')}ã€‘"
                lines.append(f"  {label}{i}. {tag}{c['suggested_title']}{source} {hot_mark}{hint}{ref}")
        else:
            lines.append("  ï¼ˆæš‚æ— å€™é€‰ï¼‰")

        lines.append("")
    
    lines.append("ğŸ’¬ è¯·å›å¤é€‰æ‹©ï¼Œå¦‚ï¼šA1,B3ï¼ˆæ¯ä¸ªè´¦å·é€‰ä¸€ç¯‡ï¼Œå³å¼€å§‹ç”Ÿæˆï¼‰")
    return "\n".join(lines)


def format_auto_message(accounts: dict, auto_count: int = 3) -> str:
    """æ ¼å¼åŒ–è‡ªåŠ¨æ¨¡å¼æ¶ˆæ¯ï¼ˆå·²è‡ªåŠ¨é€‰æ‹© top Nï¼Œå¾…ç¡®è®¤æ¨é€ï¼‰ã€‚"""
    lines = [f"ğŸ¤– è‡ªåŠ¨é€‰é¢˜å®Œæˆï¼ˆ{datetime.now().strftime('%mæœˆ%dæ—¥')}ï¼‰\n"]
    lines.append(f"å·²ä¸ºæ¯ä¸ªè´¦å·è‡ªåŠ¨é€‰æ‹© Top {auto_count} è¯é¢˜å¹¶ç”Ÿæˆæ–‡ç« ï¼š\n")
    
    for label, data in accounts.items():
        acc_name = data["account_name"] or data["account_id"]
        lines.append(f"ã€{label}ã€‘{acc_name}")
        for i, c in enumerate(data["candidates"][:auto_count], 1):
            source = f"ï¼ˆ{c['source']}ï¼‰" if c.get("source") else ""
            ref = f"\n      å‚è€ƒï¼š{c['original_title']}" if c.get("original_title") else ""
            lines.append(f"  {label}{i}. {c['suggested_title']}{source}{ref}")
        lines.append("")
    
    lines.append("ğŸ’¬ è¯·å›å¤è¦æ¨é€çš„æ–‡ç« ï¼Œå¦‚ï¼šA1,B2ï¼ˆé€‰å®šçš„å°†æ¨é€åˆ°å¯¹åº”å¹³å°ï¼‰")
    lines.append("å›å¤ã€Œå…¨éƒ¨ã€= å…¨éƒ¨æ¨é€")
    return "\n".join(lines)


def parse_selection(text: str, accounts: dict) -> list:
    """è§£æç”¨æˆ·é€‰æ‹©å›å¤ã€‚
    
    è¾“å…¥å¦‚ "A1,B3" æˆ– "A1 B3" æˆ– "å…¨éƒ¨"
    
    Returns: [{"label": "A", "index": 0, "account_id": str, "title": str}, ...]
    """
    text = text.strip()
    
    if text in ("å…¨éƒ¨", "all", "ALL"):
        result = []
        for label, data in accounts.items():
            for i, c in enumerate(data["candidates"]):
                result.append({
                    "label": label,
                    "index": i,
                    "account_id": data["account_id"],
                    "account_name": data["account_name"],
                    "platform": data["platform"],
                    "title": c["suggested_title"],
                    "url": c.get("url", ""),
                })
        return result
    
    import re
    selections = re.findall(r'([A-Za-z])(\d+)', text)
    result = []
    for label, num in selections:
        label = label.upper()
        idx = int(num) - 1
        if label in accounts and 0 <= idx < len(accounts[label]["candidates"]):
            c = accounts[label]["candidates"][idx]
            result.append({
                "label": label,
                "index": idx,
                "account_id": accounts[label]["account_id"],
                "account_name": accounts[label]["account_name"],
                "platform": accounts[label]["platform"],
                "title": c["suggested_title"],
                "url": c.get("url", ""),
                "original_title": c.get("original_title", ""),
                "source": c.get("source", ""),
                "search_suggested": bool(c.get("search_suggested")),
                "rank": c.get("rank"),
            })
    return result


if __name__ == "__main__":
    # CLI test
    result = run_autotopic()
    if result.get("error"):
        print(f"Error: {result['error']}")
    else:
        print(result["message"])
        print(f"\n--- Total hot items: {result['total_hot']}, Date: {result['date']} ---")
